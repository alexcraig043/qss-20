{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set 3: Merging and regular expressions\n",
    "\n",
    "**Total points (without extra credit)**: 44\n",
    "\n",
    "**Background on the policy context**: here, we're going to use two datasets to practice reshaping, merging, and regular expression patterns. Both datasets relate to the broader issue of which employers might be violating the rights of temporary guestworkers granted visas under the H-2A program. Here are some articles about potential exploitation of guestworkers by firms and inequality caused by minimal oversight:\n",
    "\n",
    "- News media coverage of labor abuses of temporary guestworkers: https://www.buzzfeednews.com/article/kenbensinger/the-pushovers \n",
    "- GAO report on labor abuses of temporary guestworkers: https://www.gao.gov/products/gao-15-154\n",
    "\n",
    "The following datasets are located in `pset3_inputdata` (need to unzip): \n",
    "\n",
    "- `jobs`: a dataset of guestworker jobs posted by many employers, some of whom have been debarred (banned) from the program for labor abuses; others not debarred\n",
    "- `debar`: a dataset of employers who committed violations of labor regulations meant to protect temporary guestworkers \n",
    "\n",
    "\n",
    "## Resources from class\n",
    "\n",
    "- [Lecture](https://github.com/jhaber-zz/QSS20_public/blob/main/slides/06_qss20_w23_mergereshape.pdf) and [activity](https://github.com/jhaber-zz/QSS20_public/blob/main/activities/03_reshaping_merging_solutions.ipynb) on exact merging\n",
    "- [Lecture](https://github.com/jhaber-zz/QSS20_public/blob/main/slides/07_qss20_w23_regex.pdf) and [activity](https://github.com/jhaber-zz/QSS20_public/blob/main/activities/04_regex_blank.ipynb) on regular expressions (both coming in class on Feb. 1)\n",
    "- DataCamp modules on both of these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Load packages & data (1 point total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## helpful packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "## repeated printouts\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Load data (0 points)\n",
    "\n",
    "Load the following dataset stored in `pset3_inputdata`&mdash;`debar.csv`&mdash;storing it as a dataframe named `debar`. This represents employers temporarily banned from hiring workers.\n",
    "\n",
    "View the `head()` and columns of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most notable column names mean as follows:\n",
    "- `Name`:\tCompany name of agricultural employer\n",
    "- `City, State`:\tCity and state where employer located\n",
    "- `Violation`:\tType of program violation\n",
    "- `Start date`:\tStart date of debarment (temporary ban)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Inspect data (1 point)\n",
    "\n",
    "Print the number of rows in `debar` versus the number of unique employer names (`Name`). Is there one row per employer or multiple rows for some employers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reshape data and check duplicates (19 points total)\n",
    "\n",
    "## 1.1 Make indicator for violation number (2 points)\n",
    "\n",
    "To make it possible to reshape data, make an indicator for the violation number for each business. The indicator should take the value of `viol` if it's the first row/potential violation, `viol2` if the second row/potential violation, etc.\n",
    "\n",
    "**Hint:**\n",
    "- One way to do this is by using an if-else statement to check whether the business name is the same as in the row above (assuming rows are ordered by name). Grouping by employer name and checking the number of unique offenses would also work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Clean up state names (3 points)\n",
    "\n",
    "Inspect the state names in the business violation data. Which states have names sometimes indicated in long format vs. two-letter abbreviation, e.g. \"New Hampshire\" vs. \"NH\"? Which of these may have more than one violation?\n",
    "\n",
    "**Hint:** \n",
    "- One way to do this is to extract state names from the `City, State` column using regex and/or string methods.\n",
    "- The simplest way to check if a state may have more than one violation is to check how many times they appear using `value_counts()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the state names consistent with the jobs data below, convert any discrepant state names to the two-letter abbreviation format. \n",
    "\n",
    "**Hints:**\n",
    "- You could do this for all state names (to be safe) or only those you just identified. \n",
    "- If you want to change ANY discrepant state names from long format to two-letter format (i.e., a complete conversion), you can load state names from a complete name/abbreviation crosswalk and use that to change state names. Here is code to load in such a list ([from this blog](https://towardsdatascience.com/state-name-to-state-abbreviation-crosswalks-6936250976c)):\n",
    "```python\n",
    "crosswalk_url = 'http://app02.clerk.org/menu/ccis/Help/CCIS%20Codes/state_codes.html'\n",
    "pd.read_html(crosswalk_url)[0]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Investigate duplicated rows (4 points)\n",
    "\n",
    "A. Create a new column in `debar`, `is_repeated`, that tells us whether an employer (`Name`) is repeated > 1 times.\n",
    "\n",
    "*Hint*: there are multiple ways to solve this but some possibilities to get the list of names that are repeated are:\n",
    "- Using `value_counts()` on the `Name` variable and extracting the index from that value counts \n",
    "- Using groupby to count the rows attached to one name\n",
    "\n",
    "B. Print the rows where `is_repeated == True` and interpret with at least a sentence. If you notice any cases of duplicate business names where 'City, State' does NOT match exactly, check these manually and make them consistent.\n",
    "\n",
    "C. Subset to the rows where `is_repeated == True` and save that data as `mult_debar`. Print the head() and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Reshape `mult_debar` to wide format to begin filtering out duplicates (4 points)\n",
    "\n",
    "You want to separate out two cases:\n",
    "\n",
    "- Cases where the repeat rows for one employer are due to duplicated data \n",
    "- Cases where the repeat rows for one employer represent repeated violations for different issues\n",
    "\n",
    "There are various ways to check duplicates in these data: e.g., converting `Violation` to lowercase or replacing spelled-out states with two-dig state codes.\n",
    "\n",
    "We're going to use the simple rule of:\n",
    "\n",
    "- A row is a duplicate if, within an employer (defined by Name + City, State), the Start date for each row's violation is the same \n",
    "\n",
    "To begin to check this, reshape `mult_debar` to a wide dataframe (`mult_debar_wide`) with the following columns, treating the `Name` and `City, State` as the index for the pivot:\n",
    "\n",
    "- Name\n",
    "- City, State\n",
    "- start_date_viol1\n",
    "- start_date_viol2\n",
    "\n",
    "For a successful reshaping, make sure each row in `mult_debar` shows only a single business (no duplicates under `Name`) and isn't missing either `start_date_viol1` or `start_date_viol2`.\n",
    "\n",
    "Show the contents of `mult_debar` and its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Filter out duplicates from original debar data (6 points)\n",
    "\n",
    "A. Using `mult_debar_wide`, add a column `is_dup` that takes value of True for cases where start_date_viol1 == start_date_viol2 marking the row as a duplicate\n",
    "\n",
    "B. Going back to the original long-format data you loaded at the beginning, `debar`, filter as follows:\n",
    "- For employers where `is_dup == True` as indicated by your wide-format dataframe, only keep `violnum == viol1`\n",
    "- For all other employers (so is_dup == False and ones we didnt need to check duplicates for), keep all violnum\n",
    "- Remove the `is_repeated` column from the `debar` data\n",
    "\n",
    "**Hint**: you can complete part B without a for loop; `pd.concat` with axis = 0 (row binding) is one way\n",
    "\n",
    "Call the resulting dataframe `debar_clean` and print the shape and # of unique employer names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Merging and regex (19 points total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load data on job postings (1 point)\n",
    "\n",
    "The previous dataset contains a small subset of employers who faced temporary bans due to violations of H-2A program regulations. Since most of the bans have expired, let's see which of those employers posted new H-2A jobs in the first quarter of 2021.\n",
    "\n",
    "First, load the `jobs.csv` data stored in `pset3_inputdata`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here to load the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most notable column names mean as follows:\n",
    "- `CASE_NUMBER`:\tAdministrative identifier for an employer's H-2A visa application\n",
    "- `EMPLOYER_NAME`:\tEmployer name\n",
    "- `EMPLOYER_CITY`:\tEmployer city\n",
    "- `EMPLOYER_STATE`:\tEmployer state\n",
    "- `EMPLOYER_ADDRESS_1`:\tEmployer address (only need to use if doing the geocoding extra credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  2.2 Try inner join on employer name  (2 points)\n",
    "\n",
    "- Use the `EMPLOYER_NAME` field of the `jobs` dataset\n",
    "- Use the `Name` field of the `debar_clean` dataset \n",
    "\n",
    "A. Use pd.merge with an inner join on those fields to see whether there are any exact matches. \n",
    "\n",
    "B. If there are exact matches, print the row(s) with exact matches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Targeted regex (11 points total)\n",
    "\n",
    "You want to see if you can increase the exact match rate with some basic cleaning of each \n",
    "of the employer name fields in each dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.1 Convert to upper (3 points)\n",
    "\n",
    "A. Convert the `EMPLOYER_NAME` and `Name` fields to uppercase using list comprehension rather than df.varname.str.upper() (it's fine to do a separate list comprehension line for each of the two columns)\n",
    "\n",
    "B. Print a random sample of 15 values of each result\n",
    "\n",
    "C. Assign the full vector of uppercase names back to the original data, writing over the original `EMPLOYER_NAME` and `Name` columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code to turn into uppercase here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code for the random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code for assigning the uppercase names back to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Clean up punctuation (4 points)\n",
    "\n",
    "You notice that INC, CO, and LLC are sometimes followed by a period (.) but sometimes not\n",
    "\n",
    "A. For each dataset, write a regex pattern using `re.sub` to remove the . but only if it's preceded by INC, LLC, or CO \n",
    "\n",
    "Make sure LLC, INC, CO remain part of the string but just without the dot\n",
    "\n",
    "B. Test the pattern on the positive and negative example we provide below and print the result. See the Github issue for examples of what to return\n",
    "\n",
    "\n",
    "**Hint**: https://stackoverflow.com/questions/7191209/python-re-sub-replace-with-matched-content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example_1 = \"CISCO PRODUCE INC.\"\n",
    "pos_example_2 = \"AVOYELLES HONEY CO., LLC\"\n",
    "neg_example = \"E.V. RANCH LLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## insert your code here with the regex pattern for part A\n",
    "\n",
    "## insert your code to use re.sub to apply the pattern to the test cases for part B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Clean employer names (4 points)\n",
    "\n",
    "Use that pattern in conjunction with `re.sub` and list comprehension to clean the employer name columns in each dataset. Save the new columns as `name_clean` in each. Then, use row subsetting to (1) subset to rows that changed names and (2) for:\n",
    "\n",
    "- `debar_clean` print the `Name` and `name_clean` columns\n",
    "- `jobs` print the `EMPLOYER_NAME` and `name_clean` columns\n",
    "\n",
    "Make sure to use the uppercase versions of the variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to clean the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here to print the head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 More joins and more cleaning (5 points)\n",
    "\n",
    "A. Conduct another inner join between `jobs` and `debar_clean` now using the `name_clean` column; print the result. Did the cleaning result in any more employers matched between the two datasets?\n",
    "\n",
    "B. Create a new column in `debar_clean` called `name_clean_2` that uses regex to take the following name in that dataset:\n",
    "\n",
    "- `SLASH E.V. RANCH LLP` in the `debar_clean` dataset\n",
    "\n",
    "And cleans it up so that it matches with this employer in `jobs`\n",
    "\n",
    "- `SLASH EV RANCH` in the `jobs` dataset\n",
    "\n",
    "Eg a pattern to remove the dots in the EV and the space+LLP-- you can apply the pattern to all employer names in debar_clean (so don't need to worry about only applying it to that one employer)\n",
    "\n",
    "\n",
    "C. Conduct a left join using `name_clean_2` as the join column where the left hand dataframe is `jobs`; right hand dataframe is `debar_clean`, store the result as a dataframe, and print the rows where the merge indicator indicates the row was found in both dataframe. Write a sentence describing how name cleaning affected the match results.\n",
    "\n",
    "**Note**: this manual cleaning process is inefficient and may miss other likely matches. A better approach would be fuzzy matching, which would recognize that 'Slash EV ranch' is a highly similar string to 'slash ev ranch llp' and match them without us needing to use regex to make the strings identical. We may talk about this in class later if time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(your interpretation here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regex to separate companies from individuals (6 points)\n",
    "\n",
    "You notice some employers in `debar_clean` have both the name of the company and the name of individual, e.g.:\n",
    "    \n",
    "COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\n",
    "\n",
    "Use the uppercase/cleaned `name_clean` in `debar_clean`\n",
    "\n",
    "A. Write a regex pattern that does the following:\n",
    "    - Captures the pattern that occurs before COMPANY if (COMPANY) is in string; so in example above, extracts COUNTY FAIR FARM \n",
    "    - Captures the pattern that occurs before INDIVIDUAL if (INDIVIDUAL) is also in string -- so in above, extracts ANDREW WILLIAMSON (so omit the \"and\")\n",
    "    \n",
    "B. Test the pattern on `pos_example` and `neg_example`-- make sure former returns a list (if using find.all) or match object (if using re.search) with the company name and individual name separated out; make sure latter returns empty\n",
    "    \n",
    "**Hints and resources**: for step A, you can either use re.search, re.match, or re.findall; don't worry about matching B&R Harvesting and Paul Cruz (Individual)\n",
    "\n",
    "- Same regex resources as above\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_example = \"COUNTY FAIR FARM (COMPANY) AND ANDREW WILLIAMSON (INDIVIDUAL)*\"\n",
    "neg_example = \"CISCO PRODUCE INC\"\n",
    "\n",
    "## your code here to define the pattern\n",
    "\n",
    "## your code here to apply it to the pos_example\n",
    "\n",
    "## your code here to apply it to the negative example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Iterate over the `name_clean` column in debar and use regex to create two new columns in `debar_clean`:\n",
    "   - `co_name`: A column for company (full `name_clean` string if no match; pattern before COMPANY if one extracted)\n",
    "   - `ind_name`: A column for individual (full `name_clean` string if no match; pattern before INDIVIDUAL if one extracted)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "D. Print three columns for the rows in `debar_clean` containing the negative example and positive example described above (county fair farm and cisco produce):\n",
    "\n",
    "- `name_clean`\n",
    "- `co_name`\n",
    "- `ind_name`\n",
    "- `Violation`\n",
    "\n",
    "**Note**: as shown in the outcome there may be duplicates of the same company reflecting different violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Optional extra credit: Geospatial visualization (2 points)\n",
    "\n",
    "Geocode the employer addresses in `jobs` and plot the addresses of jobs as points overlaid on a map of Georgia. This involves Googling and using external sources to figure out the code (a common practice in real-life data science), since we haven't spatial data in the course. \n",
    "\n",
    "**Hints:**\n",
    "- Relevant columns include `EMPLOYER_ADDRESS_1` \n",
    "- The geocoding might have a long runtime, so feel free to implement it in a separate `.py` script that you submit alongside your notebook and to just read in the geocoded data\n",
    "\n",
    "**Resources:**\n",
    "- [Discussion of geocoding addresses -> lat/long](https://www.natasshaselvaraj.com/a-step-by-step-guide-on-geocoding-in-python/)\n",
    "- [Discussion of plotting lat/long dots against a map using geopandas](https://towardsdatascience.com/plotting-maps-with-geopandas-428c97295a73)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:25:23) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
