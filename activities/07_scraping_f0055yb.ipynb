{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to web-scraping\n",
    "\n",
    "## Outline\n",
    "\n",
    "* [Making `Requests`](#request)\n",
    "* [Parsing HTML](#parsing)\n",
    "    * [Pretty parsing with `BeautifulSoup`](#BS)\n",
    "    * [Getting human-readable text](#readable)\n",
    "* [URL collection with automated Google search](#URLs)\n",
    "    * [Scraping school URLs](#school_URLs)\n",
    "    * [Scraping URLs using an exclusion list](#exclusionlist)\n",
    "\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 693,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for parsing\n",
    "from bs4 import BeautifulSoup # essential package for parsing in Python\n",
    "import requests # for web requests\n",
    "\n",
    "# for automated URL collection\n",
    "# !pip install google # needs running only once\n",
    "from googlesearch import search # automated Google search package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making `Requests` <a id='request'></a>\n",
    "\n",
    "The first step in web-scraping is getting the HTML of the website we want to scrape. The [requests](http://docs.python-requests.org/en/master/) library is the easiest way to do this in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Abenaki'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, it looks like everything worked! Let's see our beautiful HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the `requests.get` function returned (and the thing in our `response` variable) was a Response object. It itself isn't the HTML that we wanted, but rather a collection of metadata about the request/response interaction between your computer and the Wikipedia server.\n",
    "\n",
    "For example, it knows whether the response was successful or not (`response.ok`), how long the whole interaction took (`response.elapsed`), what time the request took place (`response.headers['Date']`) and a whole bunch of other metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 696,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 696,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wed, 15 Feb 2023 20:39:01 GMT'"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.headers['Date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, what we really care about is the HTML content. We can get that from the `Response` object with `response.text`. What we get back is a string of HTML, exactly the contents of the HTML file at the URL that we requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-disabled vector-feature-page-tools-pinned-disabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled\" lang=\"en\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"UTF-8\"/>\n",
      "<title>Abenaki - Wikipedia</title>\n",
      "<script>document.documentElement.className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-language-alert-in-sidebar-enabled vector-feature-sticky-header-disabled vector-feature-page-tools-disabled vector-feature-page-tools-pinned-disabled vector-feature-main-menu-pinned-disabled vector-feature-limited-width-enabled vector-feature-limited-width-content-enabled\";(function(){var cookie=document.cookie.match(/(?:^|;\n"
     ]
    }
   ],
   "source": [
    "html = response.text\n",
    "print(html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Get the HTML for [this claim review by fact checking site PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/). \n",
    "Print out the first 1000 characters and compare it to the HTML you see when you view the source HTML in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en-US\" dir=\"ltr\">\n",
      "<head>\n",
      "<meta charset=\"utf-8\">\n",
      "<meta http-equiv=\"x-ua-compatible\" content=\"ie=edge\">\n",
      "<meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n",
      "<title>PolitiFact | Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.</title>\n",
      "<meta name=\"description\" content=\"Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal. Senate Minority Leader \" />\n",
      "<meta property=\"og:url\" content=\"https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\" />\n",
      "<meta property=\"og:image\" content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" />\n",
      "<meta property=\"og:image:secure_url\" content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" />\n",
      "<meta property=\"og:title\" content=\"PolitiFact - Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\" />\n",
      "<meta propert\n"
     ]
    }
   ],
   "source": [
    "# your solution here\n",
    "biden_url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "biden_response = requests.get(biden_url)\n",
    "\n",
    "biden_html = biden_response.text\n",
    "print(biden_html[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsing HTML <a id='parsing'></a>\n",
    "\n",
    "After getting HTML (e.g., with `Requests`), the second step in web-scraping is parsing HTML. This is where things can get a little tricky.\n",
    "\n",
    "Let's start by looking more closely at HTML. Use your browser developer tools (e.g., in Chrome, right click > `Inspect`) to inspect the HTML of [the page listing all courses in Quantitative Social Science in 2022-23 at Dartmouth College](https://dartmouth.smartcatalogiq.com/en/current/orc/Departments-Programs-Undergraduate/Quantitative-Social-Science/QSS-Quantitative-Social-Sciences) in your browser, and find the HTML of the hyperlink to each specific course page. There's a lot of other stuff in the file that we don't care too much about. You could try `Crtl-F`ing for the name of a course you see on the webpage (you might have to then scroll around to find the highlighted code).\n",
    "\n",
    "You should see listings like these ones:\n",
    "\n",
    "```html\n",
    "<ul class=\"sc-child-item-links\">\n",
    "    <li>\n",
    "        <a href=\"/en/current/orc/Departments-Programs-Undergraduate/Quantitative-Social-Science/QSS-Quantitative-Social-Sciences/QSS-15\">\n",
    "            QSS&nbsp;15&nbsp;Introduction to Data Analysis\n",
    "        </a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"/en/current/orc/Departments-Programs-Undergraduate/Quantitative-Social-Science/QSS-Quantitative-Social-Sciences/QSS-17\">\n",
    "            QSS&nbsp;17&nbsp;Data Visualization\n",
    "        </a>\n",
    "    </li>\n",
    "    ...\n",
    "```\n",
    "\n",
    "This is HTML. HTML uses \"tags\", code that surrounds the raw text which indicates the structure of the content. The tags are enclosed in `<` and `>` symbols. The `<li>` says \"this is a new thing in a list\", and `</li>` says \"that's the end of that new thing in the list\". Similarly, the `<a ...>` and the `</a>` say, \"everything between us is a hyperlink\". \n",
    "\n",
    "In this HTML file, each course title is listed with `<li>...</li>` and is also linked to its own page using `<a>...</a>`. In our browser, if we click on the name of the course, it takes us to detailed information for that class, including the Instructor and Pre-Requisites. You'll see that inside the `<a>` bit, there's a `href=...`. That tells us the (relative) location of the page it's linked to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretty parsing with `BeautifulSoup` <a id='BS'></a>\n",
    "\n",
    "Armed with this knowledge of HTML, let's try getting the HTML and parsing a webpage. We will use `requests` to get the HTML and its text, then `BeautifulSoup` to parse the result. (Check out [the `BeautifulSoup` docs](http://www.crummy.com/software/BeautifulSoup/bs4/doc/) for lots of tips and tricks!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html dir=\"ltr\" lang=\"en-US\">\n",
      " <head>\n",
      "  <meta charset=\"utf-8\"/>\n",
      "  <meta content=\"ie=edge\" http-equiv=\"x-ua-compatible\"/>\n",
      "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
      "  <title>\n",
      "   PolitiFact | Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\n",
      "  </title>\n",
      "  <meta content=\"Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal. Senate Minority Leader \" name=\"description\"/>\n",
      "  <meta content=\"https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\" property=\"og:url\"/>\n",
      "  <meta content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" property=\"og:image\"/>\n",
      "  <meta content=\"https://static.politifact.com/politifact/rulings/meter-mostly-false.jpg\" property=\"og:image:secure_url\"/>\n",
      "  <meta content=\"PolitiFact - Citizens United calls Biden’s infrastructure plan the Green New Deal. It isn’t.\" property=\"og:title\"/>\n",
      "  <meta content=\"website\" property=\"og:type\"/>\n",
      "  <meta content=\"600\" property=\"og:image:width\"/>\n",
      "  <meta content=\"600\" property=\"og:image:height\"/>\n",
      "  <meta content=\"@politifact\" property=\"og\n"
     ]
    }
   ],
   "source": [
    "# Define URL to scrape: a fact-checking page\n",
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "\n",
    "# Scrape HTML\n",
    "html = requests.get(url)\n",
    "\n",
    "# Convert HTML into soup object\n",
    "soup = BeautifulSoup(html.text) # use default 'html.parser' ('lxml' is faster though)\n",
    "\n",
    "# See pretty formatting in soup object\n",
    "print(soup.prettify()[:1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty! Especially compared to the kind of raw output of `requests.get().text` we saw above. But this is just the beginning of what `BeautifulSoup` can do. It can also find specific tags, like paragraphs (via `<p>`), headers (via `h1`, `h2`, etc.), and hyperlinks (via `<a>` and their `href` elements).\n",
    "\n",
    "In most cases, the `<p>` tag is the most useful for extracting readable text from a webpage. Let's get the first 10 paragraph tags from this claim review page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>\n",
      "Our only agenda is to publish the truth so you can be an informed participant in democracy.\n",
      "<br/>We need your help.\n",
      "</p>\n",
      "<p>\n",
      "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
      "</p>\n",
      "<p class=\"c-image__caption-inner copy-xs\">\n",
      "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
      "</p>\n",
      "<p>The White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.</p>\n",
      "<p>The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.</p>\n",
      "<p>Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.</p>\n",
      "<p>Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the <a href=\"https://www.congress.gov/bill/116th-congress/house-resolution/109\">Green New Deal</a>, a sweeping environmental and social justice agenda that Republicans have condemned.</p>\n",
      "<p>\"Does this sound like an infrastructure bill to you?\" the group <a href=\"https://twitter.com/Citizens_United/status/1377308915227107336\">tweeted March 31</a>, with a link to a <a href=\"https://www.nytimes.com/live/2021/03/31/us/biden-news-today/biden-introduces-his-infrastructure-plan-calling-it-a-once-in-a-generation-investment-in-america\">New York Times</a> article about the proposal. \"It's not. It's the Green New Deal. \"</p>\n",
      "<p dir=\"ltr\" lang=\"en\">Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. <br/><br/>\"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"<a href=\"https://t.co/ajIoRCttgl\">https://t.co/ajIoRCttgl</a></p>\n",
      "<p>The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"</p>\n"
     ]
    }
   ],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Find all the links in the above claim review page using the `<a>` tags and their `href` elements. Print every 10th link. What do you notice about where these links point?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "/pennsylvania/\n",
      "/health-check/\n",
      "/personalities/\n",
      "/personalities/sean-hannity/\n",
      "/factchecks/list/?ruling=pants-fire\n",
      "/corrections-and-updates/\n",
      "/infrastructure/\n",
      "https://twitter.com/Citizens_United/status/1377308915227107336?ref_src=twsrc%5Etfw\n",
      "None\n",
      "https://www.usda.gov/media/press-releases/2021/03/10/fact-sheet-united-states-department-agriculture-provisions-hr-1319\n",
      "#\n",
      "#\n",
      "#\n",
      "/personalities/facebook-posts/\n",
      "/personalities/joe-biden/\n",
      "/personalities/mitch-mcconnell/\n",
      "/north-carolina/\n",
      "/who-pays-for-politifact/\n",
      "/copyright/\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "# Your solution here\n",
    "for link in soup.find_all('a')[::10]:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes these tags aren't very useful--in fact, they can get in the way of extracting only visible or human-readable text from the HTML. This too can be accomplished with `BeautifulSoup`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting human-readable text <a id='readable'></a>\n",
    "\n",
    "Occasionally we want to learn about websites via their tags: What the headers say, which paragraph comes first, where the links or images are, etc. Other times tags (such as scripts or styles) only introduce extraneous characters and nonsense words, and we want to ignore the tags themselves or even the text they enclose. \n",
    "\n",
    "The simplest way to do this is with the `get_text()` method in `BeautifulSoup`, which returns all the text in a document or beneath a tag, as a single Unicode string. You might have noticed that the `<p>` tags got in the way in our above example. Let's try that again and this time, we will remove the tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our only agenda is to publish the truth so you can be an informed participant in democracy.\n",
      "We need your help.\n",
      "More Info\n",
      "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
      "The White House infrastructure plan would cost about $2.3 trillion. A Green New Deal-type plan would cost $9.5 trillion.\n",
      "The Green New Deal included broader social economic goals, such as a guaranteed livable wage, affordable higher education and universal health care.\n",
      "Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal.\n",
      "Senate Minority Leader Mitch McConnell said that as written, the $2.3 trillion American Jobs Plan released March 31 was a nonstarter. The conservative PAC Citizens United put Biden’s plan in the same boat as the Green New Deal, a sweeping environmental and social justice agenda that Republicans have condemned.\n",
      "\"Does this sound like an infrastructure bill to you?\" the group tweeted March 31, with a link to a New York Times article about the proposal. \"It's not. It's the Green New Deal. \"\n",
      "Does this sound like an infrastructure bill to you? It's not. It's the Green New Deal. \"It is the first step in a two-part agenda to overhaul American capitalism, fight climate change and attempt to improve the productivity of the economy.\"https://t.co/ajIoRCttgl\n",
      "The Times article described Biden’s plan as the first step in a legislative package that aimed to boost productivity, fight climate change and \"overhaul American capitalism.\"\n"
     ]
    }
   ],
   "source": [
    "for paragraph in soup.find_all('p')[:10]: # first 10 paragraphs via <p> tag\n",
    "    print(paragraph.get_text().strip()) # extract text and strip trailing spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's also easy to call the first element of the soup object matching a given tag, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOur only agenda is to publish the truth so you can be an informed participant in democracy.\\nWe need your help.\\n'"
      ]
     },
     "execution_count": 704,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.p.get_text() # Get text of first paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful method is `extract()`, which can be used to surgically remove a tag or string from the soup tree, storing it for safe keeping. Let's extract the first 5 links:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links:\n",
      "[<a href=\"/\">\n",
      "<span class=\"m-branding\">\n",
      "<span class=\"m-branding__logo\">\n",
      "<svg class=\"c-icon\">\n",
      "<use xlink:href=\"#svg_logo-plain\"></use>\n",
      "</svg>\n",
      "</span>\n",
      "<span class=\"m-branding__subline\">\n",
      "<span class=\"m-branding__claim\">\n",
      "The Poynter Institute\n",
      "</span>\n",
      "</span>\n",
      "</span>\n",
      "</a>, <a class=\"c-burger\" data-menu-toggle=\"\" href=\"#\">\n",
      "<span class=\"c-burger__lines\"></span>\n",
      "<span class=\"c-burger__value\">Menu</span>\n",
      "</a>, <a class=\"c-button c-button--small show-for-large\" href=\"/membership/\">\n",
      "Donate\n",
      "</a>, <a href=\"/california/\">\n",
      "California\n",
      "</a>, <a href=\"/florida/\">\n",
      "Florida\n",
      "</a>]\n",
      "\n",
      "Remaining links:\n",
      "<a href=\"/illinois/\">\n",
      "Illinois\n",
      "</a>\n",
      "<a href=\"/iowa/\">\n",
      " Iowa\n",
      "</a>\n",
      "<a href=\"/missouri/\">\n",
      "Missouri\n",
      "</a>\n",
      "<a href=\"/new-york/\">\n",
      "New York\n",
      "</a>\n",
      "<a href=\"/north-carolina/\">\n",
      "North Carolina\n",
      "</a>\n"
     ]
    }
   ],
   "source": [
    "extracted = [] # initialize list of extracted links\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first five <a> tags\n",
    "    extracted.append(link.extract()) # extract the link\n",
    "    \n",
    "print('Extracted links:')\n",
    "print(extracted)\n",
    "print()\n",
    "\n",
    "# What are the first five links now that the previous five were removed? \n",
    "print('Remaining links:')\n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we don't want to keep the tag at all? In this case, we would use `decompose()`, which obliterates a useless tag (and frees up memory). Unlike with `extract()`, with `decompose()` you don't need to assign the junk tag to anything to clear it--the method does this automatically. \n",
    "\n",
    "Let's try the above code again, this time with `decompose()` and `get_text()` to clean up the display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining links:\n",
      "Illinois\n",
      "Iowa\n",
      "Missouri\n",
      "New York\n",
      "North Carolina\n"
     ]
    }
   ],
   "source": [
    "url = 'https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/'\n",
    "html = requests.get(url)\n",
    "soup = BeautifulSoup(html.text)\n",
    "\n",
    "for link in soup.find_all('a')[:5]: # get first five <a> tags\n",
    "    link.decompose() # obliterate this link\n",
    "    \n",
    "# What are the first five links now the the previous five were removed? \n",
    "print('Remaining links:')\n",
    "for link in soup.find_all('a')[:5]: \n",
    "    print(link.get_text().strip()) # get text and clean spacing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all websites use the `<p>` tag to indicate the important, human-readable text. Sometimes we need to approach HTML parsing from the other end: By finding and removing all non-informative tags. Let's use `BeautifulSoup` to build such a method. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Use `decompose()` to remove from the soup all tags showing anything other than human-readable text. Below is a list of such junk tags to use as an exclusion list. \n",
    "\n",
    "```\n",
    "\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "\"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "\"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"\n",
    "```\n",
    "\n",
    "_Hint:_ Iterate over these tags to identify each one in the soup and remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta tags pre removal:\n",
      "[<meta charset=\"utf-8\"/>, <meta content=\"ie=edge\" http-equiv=\"x-ua-compatible\"/>, <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>, <meta content=\"Republican opposition to President Joe Biden’s infrastructure proposal has been swift and vocal. Senate Minority Leader \" name=\"description\"/>, <meta content=\"https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/\" property=\"og:url\"/>]\n",
      "\n",
      "meta tags pre removal:\n",
      "[]\n",
      "\n",
      "Soup post removal:\n",
      "State EditionsIllinoisIowaMissouriNew YorkNorth CarolinaPennsylvaniaTexasVirginiaWest VirginiaVermontWisconsinMichiganIssuesAll IssuesOnline hoaxesCoronavirusHealth CareImmigrationExtremismTaxesMarijuanaEnvironmentCrimeGunsForeign PolicyLGBTQPeopleAll PeopleJoe BidenKamala HarrisCharles SchumerMitch McConnellBernie SandersNancy PelosiDonald TrumpMediaPunditFactTucker CarlsonSean HannityRachel MaddowBloggersPolitiFact VideosCampaigns2020 ElectionsTruth-o-MeterTrueMostly TrueHalf TrueMostly FalseFalsePants on FirePromisesBiden Promise TrackerTrump-O-MeterObameterLatest PromisesAbout UsOur ProcessOur StaffWho pays for PolitiFact?Advertise with UsSuggest a Fact-checkCorrections and UpdatesDonateFollow usThe Facts NewsletterSign upStand up for the facts!Our only agenda is to publish the truth so you can be an informed participant in democracy.We need your help.More InfoI would like to contributeOne TimeMonthlyYearlyJoin NowCitizens Unitedstated on March 31, 2021 in a tweet:Says Joe Biden’s \n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "remove_tags = [\"b\", \"big\", \"i\", \"small\", \"tt\", \"abbr\", \"acronym\", \"cite\", \"dfn\", \"kbd\", \n",
    "\"samp\", \"var\", \"bdo\", \"map\", \"object\", \"q\", \"span\", \"sub\", \"sup\", \"head\", \n",
    "\"title\", \"[document]\", \"script\", \"style\", \"meta\", \"noscript\"]\n",
    "\n",
    "print(\"meta tags pre removal:\")\n",
    "print(soup.find_all('meta')[:5])\n",
    "\n",
    "for tag_type in remove_tags:\n",
    "    for tag in soup.find_all(tag_type):\n",
    "        tag.decompose()\n",
    "        \n",
    "print(\"\\nmeta tags pre removal:\")\n",
    "print(soup.find_all('meta'))\n",
    "\n",
    "print(\"\\nSoup post removal:\")\n",
    "print(soup.get_text(strip=True)[:1000])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that word boundaries get clobbered when you call `get_text()`. This is because the default setting for this method is `strip=True`, which tells `BeautifulSoup` to strip whitespaces (of any kind) from the beginning and end of each bit of text. Using `strip=False` leads to lots of extra whitespaces--usually, newlines--which requires some regular expressions to clean up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Using the above tags exclusion list and `decompose()` as before, this time use the `strip=False` parameter when calling `get_text()` to avoid combining words across whitespace boundaries. Instead, use regular expressions to clean up extra whitespaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " State Editions Illinois Iowa Missouri New York North Carolina Pennsylvania Texas Virginia West Virginia Vermont Wisconsin Michigan Issues All Issues Online hoaxes Coronavirus Health Care Immigration Extremism Taxes Marijuana Environment Crime Guns Foreign Policy LGBTQ People All People Joe Biden Kamala Harris Charles Schumer Mitch McConnell Bernie Sanders Nancy Pelosi Donald Trump Media PunditFact Tucker Carlson Sean Hannity Rachel Maddow Bloggers PolitiFact Videos Campaigns 2020 Elections Truth-o-Meter True Mostly True Half True Mostly False False Pants on Fire Promises Biden Promise Tracker Trump-O-Meter Obameter Latest Promises About Us Our Process Our Staff Who pays for\n"
     ]
    }
   ],
   "source": [
    "# Your solution here    \n",
    "import re\n",
    "\n",
    "text_nostrip = soup.get_text(strip=False)\n",
    "\n",
    "regex = r'\\s+'\n",
    "text_processed = re.sub(regex, ' ', text_nostrip[:1000])\n",
    "\n",
    "print(text_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Challenge\n",
    "\n",
    "You might have noticed that when we scraped HTML above from [this claim review by PolitiFact](https://www.politifact.com/factchecks/2021/apr/02/citizens-united/citizens-united-calls-bidens-infrastructure-plan-g/), we got headers and tags like this:\n",
    "```html\n",
    "<p>Misinformation isn't going away just because it's a new year. Support trusted, factual information with a tax deductible contribution to PolitiFact.</p>\n",
    "<p>\n",
    "<a class=\"m-disruptor-content__link\" href=\"/membership/\">More Info</a>\n",
    "</p>\n",
    "<p class=\"c-image__caption-inner copy-xs\">\n",
    "The White House infrastructure plan has $111 billion to improve water and sewer systems. (Shutterstock)\n",
    "</p>\n",
    "```\n",
    "Use what you now know about identifying HTML, removing tags, and cleaning spacing to scrape a clean explanation from the body of this article. \n",
    "\n",
    "_Hint:_ Use your browser to inspect this website's HTML and identify any unique types and/or classes that enclose the explanation (and nothing else)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the output from this focused, site-specific scraping approach with that from the exclusion list method above. <br/>\n",
    "**Which method gives the cleaner output? Which method is more extensible?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL collection with automated Google search<a id='URLs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to crawl and/or scrape an online community of websites, there's a good chance may find yourself needing to collect their URLs. If you're lucky, you have comprehensive metadata describing these entities, something like their name and physical address. Your next step in this scenario would be to automate a Google search to collect the best URL matching each entity. \n",
    "\n",
    "How can you scrape URLs from Google? There are two fairly easy ways.\n",
    "\n",
    "First, the **Google Places API**, which is the best option to do this at scale. You would need to apply for an API key from Google: go to the [Google cloud console](https://console.cloud.google.com/), create a project, and request an API key for each service you want to use. Approval may take a few days, but once done there is a [handy Python wrapper](https://github.com/slimkrazy/python-google-places) to make this easy to use in Python. See [Google Web Services](https://developers.google.com/places/web-service/) for general documentation and [Google Developers](https://developers.google.com/places/web-service/details) for details on Place Details requests.\n",
    "\n",
    "The second option is **automated Google search**, which is not nearly as reliable and may get you blocked if used repeatedly. This method tends to get lots of false positives and third-party website aggregators (e.g., yellowpages.com, trulia.com), so using an exclusion list to manually filter results is a good idea. Check out [the source code](https://github.com/MarioVilas/googlesearch) and [documentation](https://python-googlesearch.readthedocs.io/en/latest/). _Thanks Mario Vilas for this package!_\n",
    "\n",
    "Because this second option is free and has no waiting period to use, we will practice using this in a nice way. In case you want to pursue further the first option, at the bottom of this notebook there is template code for running the Google Places API.\n",
    "\n",
    "_Note_: Remember what I said about following the Terms of Service for APIs? You might find real gems in there--like this extract from the [Google Maps Platform Terms of Service](https://developers.google.com/terms/) that prohibits scraping data you intend to store:\n",
    "\n",
    "```\n",
    "3.2.3 Restrictions Against Misusing the Services.\n",
    "\n",
    "(a)  No Scraping. Customer will not export, extract, or otherwise scrape Google Maps Content for use outside the Services. For example, Customer will not: (i) pre-fetch, index, store, reshare, or rehost Google Maps Content outside the services; (ii) bulk download Google Maps tiles, Street View images, geocodes, directions, distance matrix results, roads information, places information, elevation values, and time zone details; (iii) copy and save business names, addresses, or user reviews; or (iv) use Google Maps Content with text-to-speech services.\n",
    "```\n",
    "\n",
    "Keep this in mind should you consider using the Google Places API for URL scraping (as my template code below does): The same terms apply, so be nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping school URLs<a id='school_URLs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this works, let's start by searching for the best URL for a charter school in Washington, D.C. Assume we have the name and address of the school.\n",
    "\n",
    "To prevent overwhelming Google search with rapid requests--and likely getting our IP address blocked by Google as a result--let's search only for the first 10 results and include a five-second pause in between each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ccpcs.org/\n",
      "https://www.ccpcs.org/about/our-staff\n",
      "https://www.ccpcs.org/about/our-staff/join-our-team\n",
      "https://www.ccpcs.org/current-families/calendar\n",
      "https://www.ccpcs.org/about/our-staff/high-school\n",
      "https://www.niche.com/k12/capital-city-public-charter-school-washington-dc/\n",
      "https://www.myschooldc.org/schools/profile/143\n",
      "https://www.myschooldc.org/schools/profile/142\n",
      "https://www.usnews.com/education/k12/district-of-columbia/capital-city-pcs-lower-school-226373\n",
      "https://nces.ed.gov/ccd/schoolsearch/school_detail.asp?Search=1&ID=110003500475\n"
     ]
    }
   ],
   "source": [
    "# Define metadata for a single entity: a DC charter school\n",
    "school_name = 'Capital City Public Charter School'\n",
    "school_address = '100 Peabody Street NW, Washington, DC 20011'\n",
    "\n",
    "# Search for first 10 Google results using joined metadata, show each one\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=10, pause=2.5):\n",
    "    print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pretty strong result: the first six matches share the domain of https://www.ccpcs.org/, so this is probably the best match. We identified a URL without even visiting any websites!\n",
    "\n",
    "Notice that results 7-10 are about the right school, but they don't point to it's genuine website--with all its descriptive language, images, and subpages. Even in this case with a strong topline result, we can already get a feel for what websites will pollute our automated searches: Facebook and greatschools.org are a good start to making an exclusion list to filter the results. \n",
    "\n",
    "Now let's try something harder to find."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the first 10 results from Google for Dr. David C. Walker Intermediate School located at 6500 Ih 35 N Ste C, San Antonio, TX 78218. What do you notice about the results? How do they compare to the previous set of results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "https://www.usnews.com/education/k12/texas/dr-david-c-walker-elementary-206298\n",
      "https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "https://www.mapquest.com/us/texas/dr-david-c-walker-intermediate-school-438581037\n",
      "https://www.mapquest.com/us/texas/dr-david-c-walker-int-475545773\n",
      "https://www.publicschoolreview.com/dr-david-c-walker-elementary-school-profile\n",
      "http://www.trueschools.com/schools/texas/san-antonio/dr-david-c-walker-intermediate/\n",
      "https://www.hisawyer.com/listings/providers/126444-dr-david-c-walker-elementary\n",
      "https://www.dnb.com/business-directory/company-profiles.school_of_excellence_in_education.8fde8b90005cb3de714dd31c0d8e98f4.html\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "\n",
    "# Search for first 10 Google results using joined metadata, show each one\n",
    "for url in search(school_name + ' ' + school_address, \\\n",
    "                  stop=10, pause=2.5):\n",
    "    print(url)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are much less clear and organized: Each one points to a different site, and all of them are third parties. Why would this be the case? \n",
    "\n",
    "This school site probably has poor SEO (search engine optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping URLs using an exclusion list<a id='exclusionlist'></a>\n",
    "\n",
    "To provide cleaner search results, let's filter out the third-party websites from the previous two examples. \n",
    "\n",
    "Many of these websites can show up with either 'http' or 'https', often with or without a 'www', but usually have a consistent top-level domain (e.g., 'com'). Exact string matchin would fail to capture matches across these variations. Regular expressions could do this, but for now let's just filter out those search results that contain the core of any domain name in the exclusion list (e.g., niche.com). \n",
    "\n",
    "Let's get the first result for the previous school (Dr. David C. Walker Intermediate School) that doesn't match any domains in the exclusion list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected Google search results.\n",
      "Bad site detected: https://www.niche.com/k12/dr-david-c-walker-intermediate-school-san-antonio-tx/\n",
      "Bad site detected: https://elementaryschools.org/directory/tx/cities/san-antonio/dr-david-c-walker-elementary/480006211404/\n",
      "Bad site detected: https://www.usnews.com/education/k12/texas/dr-david-c-walker-elementary-206298\n",
      "Bad site detected: https://www.greatschools.org/texas/san-antonio/12035-Dr-David-C-Walker-Intermediate-School/\n",
      "Success! URL obtained by Google search with 4 bad URLs avoided.\n",
      "Quality URL: https://www.mapquest.com/us/texas/dr-david-c-walker-intermediate-school-438581037\n"
     ]
    }
   ],
   "source": [
    "# Define excluded domains to filter out: third-party domains/false positives that we DON'T want to scrape \n",
    "exclusions = ['facebook.com', 'greatschools.org', 'niche.com', 'har.com', 'usnews.com', 'publicschoolreview.com', \n",
    "             'nces.ed.gov', 'dnb.com', 'schooldigger.com', 'elementaryschools.org', 'closelocation.com']\n",
    "\n",
    "# Define search metadata\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "#school_name = \"River City Scholars Charter Academy\"\n",
    "#school_address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "# Collect search results\n",
    "urls = search(school_name + ' ' + school_address, \\\n",
    "              stop=20, pause=5.0) # Expand search range to help avoid excluded domains\n",
    "print(\"Successfully collected Google search results.\")\n",
    "\n",
    "# Initialize exclusion list match counter: How many excluded domains has this search encountered?\n",
    "excluded_num = 0 \n",
    "\n",
    "# Loop through google search output to find first good result:\n",
    "for url in urls:\n",
    "    if any(domain in url for domain in exclusions):\n",
    "        print(f'Bad site detected: {url}') \n",
    "        excluded_num += 1 # Add one to exclusions list match counter\n",
    "    else:\n",
    "        good_url = url\n",
    "        print(\"Success! URL obtained by Google search with \" + str(excluded_num) + \" bad URLs avoided.\")\n",
    "        break # Exit for loop after first good url is found\n",
    "        \n",
    "print(f'Quality URL: {good_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of [the \"quality\" URL we landed on](https://yellow.place/en/dr-david-c-walker-int-san-antonio-tx-usa)? What does this mean about our exclusion list?\n",
    "\n",
    "### Challenge\n",
    "\n",
    "Improve our automated searching to try to get the genuine URL of Dr. David C. Walker Intermediate School. <br/>\n",
    "_Hint_: You could try (A) adding more URLs to the exclusion list OR (B) try a simple search but for more URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully collected Google search results.\n",
      "Quality URLS:\n",
      "https://www.mapquest.com/us/texas/dr-david-c-walker-intermediate-school-438581037\n",
      "https://www.mapquest.com/us/texas/dr-david-c-walker-int-475545773\n",
      "http://www.trueschools.com/schools/texas/san-antonio/dr-david-c-walker-intermediate/\n",
      "https://www.hisawyer.com/listings/providers/126444-dr-david-c-walker-elementary\n",
      "https://www.citydirectory.us/school-dr-david-c-walker-elementary-san-antonio-tx.html\n",
      "https://www.homefacts.com/schools/Texas/Bexar-County/San-Antonio/Dr-David-C-Walker-El.html\n",
      "https://wheretoteach.com/dr-david-c-walker-elementary-22789\n",
      "https://www.century21.com/schools/78218-san-antonio-tx-schools/dr-david-c-walker-intermediate-school/O10775139-LZ78218\n",
      "https://www.donorschoose.org/schools/texas/school-of-excellence-in-education/dr-david-walker-elementary-school/95612\n",
      "https://www.k12jobspot.com/District/1117/Schools\n",
      "http://www.localschooldirectory.com/public-school/345352660/TX\n",
      "http://www.loresult.com/us-zip-codes/united-states-texas-tx-78218/\n",
      "https://www.sanantonio.gov/Portals/0/Files/health/News/RestaurantReports/RRAPRIL2017.xlsx?ver=2017-05-12-162100-603\n",
      "https://magnetschools.us/county/bexar_tx\n"
     ]
    }
   ],
   "source": [
    "# Your solution here\n",
    "exclusions = ['facebook.com', 'greatschools.org', 'niche.com', 'har.com', 'usnews.com', 'publicschoolreview.com', \n",
    "             'nces.ed.gov', 'dnb.com', 'schooldigger.com', 'elementaryschools.org', 'closelocation.com']\n",
    "\n",
    "school_name = 'Dr. David C. Walker Intermediate School'\n",
    "school_address = '6500 Ih 35 N Ste C, San Antonio, TX 78218'\n",
    "#school_name = \"River City Scholars Charter Academy\"\n",
    "#school_address = \"944 Evergreen Street, Grand Rapids, MI 49507\"\n",
    "\n",
    "urls = search(school_name + ' ' + school_address, \\\n",
    "              stop=20, pause=2.5) \n",
    "print(\"Successfully collected Google search results.\")\n",
    "\n",
    "excluded_num = 0 \n",
    "good_urls = []\n",
    "\n",
    "# Loop through google search output to find all good results:\n",
    "for url in urls:\n",
    "    if any(domain in url for domain in exclusions):\n",
    "        excluded_num += 1 # Add one to exclusions list match counter\n",
    "    else:\n",
    "        good_url = url\n",
    "        good_urls.append(good_url)\n",
    "        \n",
    "good_urls_string = \"\\n\".join(good_urls)\n",
    "print(\"Quality URLS:\")\n",
    "print(good_urls_string)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
